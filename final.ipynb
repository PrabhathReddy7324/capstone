{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13859251,"sourceType":"datasetVersion","datasetId":8829065}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q \"numpy<2.0\" \"matplotlib<3.9\" \"ultralytics==8.2.50\" pyyaml\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom pathlib import Path\nimport yaml\nimport cv2\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, models\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom ultralytics import YOLO\nfrom ultralytics.utils import patches\nfrom ultralytics.nn.tasks import DetectionModel\n\n# PyTorch >=2.6 \"safe load\" fix – allow DetectionModel in checkpoints\ntry:\n    from torch.serialization import add_safe_globals\n    add_safe_globals([DetectionModel])\n    print(\"Registered DetectionModel as safe global for torch.load ✅\")\nexcept Exception as e:\n    print(\"Safe globals not needed / not available:\", e)\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", DEVICE)\n\n# ----- PATHS -----\n# Kaggle: dataset is usually mounted here:\nDATA_ROOT = Path(\"/kaggle/input/capstonev3\")\n# If you're on Colab, change DATA_ROOT to wherever you unzipped the dataset.\n\nORIG_YAML  = DATA_ROOT / \"data.yaml\"\nFIXED_YAML = Path(\"/kaggle/working/data_fixed.yaml\")  # or any writable folder\n\n# Classifier settings\nMODEL_TYPE   = \"resnet50\"   # or \"vit_b_16\"\nEPOCHS       = 20           # increase to 40–60 for max accuracy\nBATCH_SIZE   = 64\nLR           = 1e-4\nIMG_SIZE     = 224\nNUM_WORKERS  = 4            # for classifier dataloaders\n\nCLASSIFIER_CKPT = Path(\"/kaggle/working/best_classifier.pth\")\nYOLO_EXP_NAME   = \"yolov12_mar20\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T06:07:09.762502Z","iopub.execute_input":"2025-11-25T06:07:09.762802Z","iopub.status.idle":"2025-11-25T06:07:11.901328Z","shell.execute_reply.started":"2025-11-25T06:07:09.762774Z","shell.execute_reply":"2025-11-25T06:07:11.900628Z"}},"outputs":[{"name":"stdout","text":"numpy: 1.26.4\nmatplotlib: 3.7.2\nultralytics: 8.2.50\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"print(\"Original data.yaml:\")\nprint(ORIG_YAML.read_text()[:500])\n\nwith open(ORIG_YAML, \"r\") as f:\n    orig = yaml.safe_load(f)\n\nCLASS_NAMES = orig[\"names\"]\nNC = orig[\"nc\"]\nprint(\"Number of classes:\", NC)\nprint(\"Classes:\", CLASS_NAMES)\n\nfixed = {\n    \"path\": str(DATA_ROOT),     # root folder\n    \"train\": \"train/images\",\n    \"val\": \"valid/images\",\n    \"test\": \"test/images\",\n    \"nc\": NC,\n    \"names\": CLASS_NAMES,\n}\n\nwith open(FIXED_YAML, \"w\") as f:\n    yaml.safe_dump(fixed, f)\n\nprint(\"\\nUsing fixed data.yaml:\")\nprint(FIXED_YAML.read_text())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T06:07:29.259706Z","iopub.execute_input":"2025-11-25T06:07:29.260498Z","iopub.status.idle":"2025-11-25T06:07:29.265508Z","shell.execute_reply.started":"2025-11-25T06:07:29.260467Z","shell.execute_reply":"2025-11-25T06:07:29.264777Z"}},"outputs":[{"name":"stdout","text":"Patched ultralytics.utils.patches.imread ✅\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# ---- Patch Ultralytics to use cv2.imread (avoid imdecode issues) ----\ndef simple_imread(path, flags=cv2.IMREAD_COLOR):\n    return cv2.imread(str(path), flags)\n\npatches.imread = simple_imread\nprint(\"Patched ultralytics.utils.patches.imread -> cv2.imread ✅\")\n\n# ---- YOLOv12-style detector (YOLO11n backbone) ----\nyolo_model = YOLO(\"yolo11n.pt\")  # small model; you can try yolo11s.pt, yolo11m.pt, etc.\n\nyolo_model.train(\n    data=str(FIXED_YAML),\n    epochs=30,          # increase for better mAP if you have time\n    imgsz=640,\n    batch=16,\n    workers=0,          # 0 => use main process, no DataLoader worker bugs\n    project=\"yolo_mar20\",\n    name=YOLO_EXP_NAME,\n    amp=False,          # keep AMP off for stability\n)\n\nYOLO_BEST = Path(f\"yolo_mar20/{YOLO_EXP_NAME}/weights/best.pt\")\nprint(\"Best YOLO weights at:\", YOLO_BEST)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T06:07:41.663457Z","iopub.execute_input":"2025-11-25T06:07:41.663755Z","iopub.status.idle":"2025-11-25T06:07:42.516988Z","shell.execute_reply.started":"2025-11-25T06:07:41.663721Z","shell.execute_reply":"2025-11-25T06:07:42.516312Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"class YoloCropDataset(Dataset):\n    \"\"\"\n    Build a classification dataset from YOLOv12/v8/v11 txt labels.\n\n    root/\n      train/\n        images/\n        labels/\n      valid/\n        images/\n        labels/\n      test/\n        images/\n        labels/\n\n    Each line in a label file: class cx cy w h (normalized).\n    \"\"\"\n\n    def __init__(self, root_dir, split=\"train\", img_size=224, augment=False):\n        self.root_dir = Path(root_dir)\n        self.split = split\n        self.img_size = img_size\n        self.augment = augment\n\n        self.image_dir = self.root_dir / split / \"images\"\n        self.label_dir = self.root_dir / split / \"labels\"\n\n        assert self.image_dir.exists(), f\"Missing {self.image_dir}\"\n        assert self.label_dir.exists(), f\"Missing {self.label_dir}\"\n\n        self.samples = []\n        self.class_ids = set()\n        self._build_index()\n\n        normalize = transforms.Normalize(\n            mean=[0.485, 0.456, 0.406],\n            std=[0.229, 0.224, 0.225],\n        )\n\n        base = [\n            transforms.Resize((img_size, img_size)),\n            transforms.ToTensor(),\n            normalize,\n        ]\n\n        if augment:\n            self.transform = transforms.Compose(\n                [\n                    transforms.RandomHorizontalFlip(),\n                    transforms.RandomRotation(10),\n                    transforms.ColorJitter(\n                        brightness=0.2, contrast=0.2,\n                        saturation=0.2, hue=0.02\n                    ),\n                ] + base\n            )\n        else:\n            self.transform = transforms.Compose(base)\n\n        self.num_classes = max(self.class_ids) + 1\n        print(f\"[{self.split}] {len(self.samples)} crops | \"\n              f\"{len(self.class_ids)} classes.\")\n\n    def _build_index(self):\n        for label_path in sorted(self.label_dir.glob(\"*.txt\")):\n            with open(label_path, \"r\") as f:\n                lines = [l.strip() for l in f.readlines() if l.strip()]\n            if not lines:\n                continue\n\n            stem = label_path.stem\n            img_path = None\n            for ext in [\".jpg\", \".jpeg\", \".png\"]:\n                candidate = self.image_dir / f\"{stem}{ext}\"\n                if candidate.exists():\n                    img_path = candidate\n                    break\n            if img_path is None:\n                continue\n\n            for line in lines:\n                parts = line.split()\n                if len(parts) != 5:\n                    continue\n                cls_id = int(parts[0])\n                cx, cy, w, h = map(float, parts[1:])\n                self.samples.append((img_path, (cx, cy, w, h), cls_id))\n                self.class_ids.add(cls_id)\n\n        if len(self.samples) == 0:\n            raise RuntimeError(f\"No crops found in {self.label_dir}\")\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        img_path, (cx, cy, w, h), cls_id = self.samples[idx]\n        img = Image.open(img_path).convert(\"RGB\")\n        W, H = img.size\n\n        bw = w * W\n        bh = h * H\n        bx = cx * W\n        by = cy * H\n\n        x1 = int(max(0, bx - bw / 2))\n        y1 = int(max(0, by - bh / 2))\n        x2 = int(min(W, bx + bw / 2))\n        y2 = int(min(H, by + bh / 2))\n\n        if x2 <= x1 or y2 <= y1:\n            crop = img\n        else:\n            crop = img.crop((x1, y1, x2, y2))\n\n        crop = self.transform(crop)\n        return crop, cls_id\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T06:11:17.019082Z","iopub.execute_input":"2025-11-25T06:11:17.019365Z","iopub.status.idle":"2025-11-25T06:11:17.042351Z","shell.execute_reply.started":"2025-11-25T06:11:17.019342Z","shell.execute_reply":"2025-11-25T06:11:17.041304Z"}},"outputs":[{"name":"stdout","text":"Patched ultralytics.utils.patches.imread -> cv2.imread ✅\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_373/1542012879.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# ---- YOLO training (no AMP, no worker processes) ----\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0myolo_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mYOLO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"yolov8s.pt\"\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# <-- now this should load without error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m yolo_model.train(\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/models/yolo/model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, task, verbose)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;31m# Continue with default YOLO initialization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/engine/model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, task, verbose)\u001b[0m\n\u001b[1;32m    147\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     def __call__(\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/engine/model.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(self, weights, task)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuffix\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\".pt\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mckpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattempt_load_one_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"task\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moverrides\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset_ckpt_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/nn/tasks.py\u001b[0m in \u001b[0;36mattempt_load_one_weight\u001b[0;34m(weight, device, inplace, fuse)\u001b[0m\n\u001b[1;32m    853\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mattempt_load_one_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m     \u001b[0;34m\"\"\"Loads a single model weights.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m     \u001b[0mckpt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch_safe_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# load ckpt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mDEFAULT_CFG_DICT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train_args\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m  \u001b[0;31m# combine model and default args, preferring model args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mckpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ema\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mckpt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# FP32 model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/nn/tasks.py\u001b[0m in \u001b[0;36mtorch_safe_load\u001b[0;34m(weight)\u001b[0m\n\u001b[1;32m    779\u001b[0m             },\n\u001b[1;32m    780\u001b[0m         ):\n\u001b[0;32m--> 781\u001b[0;31m             \u001b[0mckpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    782\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    783\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mModuleNotFoundError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# e.name is missing module name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1468\u001b[0m                         )\n\u001b[1;32m   1469\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpicklingError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1470\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpicklingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_get_wo_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1471\u001b[0m                 return _load(\n\u001b[1;32m   1472\u001b[0m                     \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mUnpicklingError\u001b[0m: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL torch.nn.modules.container.Sequential was not an allowed global by default. Please use `torch.serialization.add_safe_globals([Sequential])` or the `torch.serialization.safe_globals([Sequential])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html."],"ename":"UnpicklingError","evalue":"Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL torch.nn.modules.container.Sequential was not an allowed global by default. Please use `torch.serialization.add_safe_globals([Sequential])` or the `torch.serialization.safe_globals([Sequential])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.","output_type":"error"}],"execution_count":7},{"cell_type":"code","source":"def create_model(num_classes: int, model_type: str = \"resnet50\"):\n    model_type = model_type.lower()\n    if model_type == \"resnet50\":\n        weights = models.ResNet50_Weights.DEFAULT\n        model = models.resnet50(weights=weights)\n        in_feats = model.fc.in_features\n        model.fc = nn.Linear(in_feats, num_classes)\n    elif model_type == \"vit_b_16\":\n        weights = models.ViT_B_16_Weights.DEFAULT\n        model = models.vit_b_16(weights=weights)\n        in_feats = model.heads.head.in_features\n        model.heads.head = nn.Linear(in_feats, num_classes)\n    else:\n        raise ValueError(\"model_type must be 'resnet50' or 'vit_b_16'\")\n    return model\n\n\ndef train_one_epoch(model, loader, criterion, optimizer, device):\n    model.train()\n    running_loss, correct, total = 0.0, 0, 0\n    for images, labels in loader:\n        images, labels = images.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item() * images.size(0)\n        _, preds = outputs.max(1)\n        correct += (preds == labels).sum().item()\n        total += labels.size(0)\n    return running_loss / total, correct / total\n\n\n@torch.no_grad()\ndef evaluate(model, loader, criterion, device):\n    model.eval()\n    running_loss, correct, total = 0.0, 0, 0\n    for images, labels in loader:\n        images, labels = images.to(device), labels.to(device)\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        running_loss += loss.item() * images.size(0)\n        _, preds = outputs.max(1)\n        correct += (preds == labels).sum().item()\n        total += labels.size(0)\n    return running_loss / total, correct / total\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Build datasets from YOLO labels\ntrain_ds = YoloCropDataset(DATA_ROOT, split=\"train\", img_size=IMG_SIZE, augment=True)\nvalid_ds = YoloCropDataset(DATA_ROOT, split=\"valid\", img_size=IMG_SIZE, augment=False)\ntest_ds  = YoloCropDataset(DATA_ROOT, split=\"test\",  img_size=IMG_SIZE, augment=False)\n\nnum_classes = train_ds.num_classes\nprint(\"Classifier num_classes:\", num_classes)\n\ntrain_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n                          num_workers=NUM_WORKERS, pin_memory=True)\nvalid_loader = DataLoader(valid_ds, batch_size=BATCH_SIZE, shuffle=False,\n                          num_workers=NUM_WORKERS, pin_memory=True)\ntest_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False,\n                          num_workers=NUM_WORKERS, pin_memory=True)\n\nclf_model = create_model(num_classes, MODEL_TYPE).to(DEVICE)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.AdamW(clf_model.parameters(), lr=LR)\n\nbest_val_acc = 0.0\n\nfor epoch in range(1, EPOCHS + 1):\n    tr_loss, tr_acc = train_one_epoch(clf_model, train_loader, criterion, optimizer, DEVICE)\n    val_loss, val_acc = evaluate(clf_model, valid_loader, criterion, DEVICE)\n    print(f\"Epoch {epoch:02d}/{EPOCHS} | \"\n          f\"train_loss={tr_loss:.4f} acc={tr_acc:.4f} | \"\n          f\"val_loss={val_loss:.4f} acc={val_acc:.4f}\")\n    if val_acc > best_val_acc:\n        best_val_acc = val_acc\n        torch.save({\n            \"model_state\": clf_model.state_dict(),\n            \"num_classes\": num_classes,\n            \"model_type\": MODEL_TYPE,\n        }, CLASSIFIER_CKPT)\n        print(\"  -> Saved new best classifier\")\n\nprint(\"Best validation accuracy:\", best_val_acc)\n\n# Load best checkpoint and evaluate on test set\nckpt = torch.load(CLASSIFIER_CKPT, map_location=DEVICE)\nbest_clf = create_model(ckpt[\"num_classes\"], ckpt[\"model_type\"]).to(DEVICE)\nbest_clf.load_state_dict(ckpt[\"model_state\"])\n\ntest_loss, test_acc = evaluate(best_clf, test_loader, criterion, DEVICE)\nprint(f\"Classifier test loss: {test_loss:.4f} | test acc: {test_acc:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# load trained YOLO detector\ndetector = YOLO(str(YOLO_BEST))\nbest_clf.eval()\n\nclf_transform = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.ToTensor(),\n    transforms.Normalize(\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225],\n    ),\n])\n\ndef classify_crop(pil_img):\n    img = clf_transform(pil_img).unsqueeze(0).to(DEVICE)\n    with torch.no_grad():\n        out = best_clf(img)\n        prob = torch.softmax(out, dim=1)\n        conf, cls = prob.max(1)\n    return cls.item(), conf.item()\n\ndef run_pipeline(image_path):\n    img = Image.open(image_path).convert(\"RGB\")\n    res = detector(str(image_path))[0]\n\n    draw = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)\n\n    for box in res.boxes:\n        x1, y1, x2, y2 = box.xyxy[0].cpu().numpy().astype(int)\n        crop = img.crop((x1, y1, x2, y2))\n\n        # classifier prediction\n        cls_id, conf_clf = classify_crop(crop)\n        cls_name = CLASS_NAMES[cls_id] if cls_id < len(CLASS_NAMES) else str(cls_id)\n\n        # YOLO prediction (for comparison)\n        yolo_cls = int(box.cls[0].item())\n        yolo_conf = float(box.conf[0].item())\n        yolo_name = CLASS_NAMES[yolo_cls] if yolo_cls < len(CLASS_NAMES) else str(yolo_cls)\n\n        label = f\"{cls_name} ({conf_clf:.2f}) | YOLO:{yolo_name} ({yolo_conf:.2f})\"\n\n        cv2.rectangle(draw, (x1, y1), (x2, y2), (0, 255, 0), 2)\n        cv2.putText(draw, label, (x1, max(0, y1-5)),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1, cv2.LINE_AA)\n\n    draw_rgb = cv2.cvtColor(draw, cv2.COLOR_BGR2RGB)\n    plt.figure(figsize=(8, 8))\n    plt.imshow(draw_rgb)\n    plt.axis(\"off\")\n    plt.title(\"YOLOv12 detector + classifier\")\n    plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample_img = next((DATA_ROOT / \"test\" / \"images\").glob(\"*\"))\nprint(\"Sample image:\", sample_img)\nrun_pipeline(sample_img)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}